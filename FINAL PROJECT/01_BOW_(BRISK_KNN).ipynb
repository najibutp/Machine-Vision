{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Copy of 01 BOW (BRISK-KNN).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy2-xzpRZCzP",
        "outputId": "78a1395d-9411-4f72-97e2-a66dabb6cf6d"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgHyjhiDZCzh"
      },
      "source": [
        "# Get the training classes names and store them in a list\n",
        "#Here we use folder names for class names\n",
        "\n",
        "#train_path = 'dataset/train' \n",
        "train_path = r'/content/drive/MyDrive/Colab Notebooks/DATA SOURCE/PPE/train'  #Google Drive\\Colab Notebooks\\DATA SOURCE\\PPE\\train\n",
        "training_names = os.listdir(train_path)\n",
        "\n",
        "# Get path to all images and save them in a list\n",
        "# image_paths and the corresponding label in image_paths\n",
        "image_paths = []\n",
        "image_classes = []\n",
        "class_id = 0\n",
        "\n",
        "#To make it easy to list all file names in a directory let us define a function\n",
        "#\n",
        "def imglist(path):    \n",
        "    return [os.path.join(path, f) for f in os.listdir(path)]\n",
        "\n",
        "#Fill the placeholder empty lists with image path, classes, and add class ID number\n",
        "#\n",
        "    \n",
        "for training_name in training_names:\n",
        "    dir = os.path.join(train_path, training_name)\n",
        "    class_path = imglist(dir)\n",
        "    image_paths+=class_path\n",
        "    image_classes+=[class_id]*len(class_path)\n",
        "    class_id+=1\n",
        "\n",
        "# Create feature extraction and keypoint detector objects\n",
        "    \n",
        "# Create List where all the descriptors will be stored\n",
        "des_list = []\n",
        "\n",
        "brisk = cv2.BRISK_create(30)\n",
        "\n",
        "for image_path in image_paths:\n",
        "    im = cv2.imread(image_path)\n",
        "    kpts, des = brisk.detectAndCompute(im, None)\n",
        "    des_list.append((image_path, des))   \n",
        "    \n",
        "# Stack all the descriptors vertically in a numpy array\n",
        "descriptors = des_list[0][1]\n",
        "for image_path, descriptor in des_list[1:]:\n",
        "    descriptors = np.vstack((descriptors, descriptor))  \n",
        "\n",
        "#kmeans works only on float, so convert integers to float\n",
        "descriptors_float = descriptors.astype(float)  \n",
        "\n",
        "# Perform k-means clustering and vector quantization\n",
        "from scipy.cluster.vq import kmeans, vq\n",
        "\n",
        "k = 300  #k means with 100 clusters gives lower accuracy for the aeroplane example\n",
        "voc, variance = kmeans(descriptors_float, k, 1) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLeGYmBoZCzj"
      },
      "source": [
        "# Calculate the histogram of features and represent them as vector\n",
        "#vq Assigns codes from a code book to observations.\n",
        "im_features = np.zeros((len(image_paths), k), \"float32\")\n",
        "for i in range(len(image_paths)):\n",
        "    words, distance = vq(des_list[i][1],voc)\n",
        "    for w in words:\n",
        "        im_features[i][w] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM9weWmwZCzk"
      },
      "source": [
        "# Perform Tf-Idf vectorization\n",
        "nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
        "idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
        "\n",
        "# Scaling the words\n",
        "#Standardize features by removing the mean and scaling to unit variance\n",
        "#In a way normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "stdSlr = StandardScaler().fit(im_features)\n",
        "im_features = stdSlr.transform(im_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUq59Sp_ZCzl"
      },
      "source": [
        "#Train an algorithm to discriminate vectors corresponding to positive and negative training images\n",
        "# Train the KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors=5)  #Default of 100 is not converging\n",
        "clf.fit(im_features, np.array(image_classes))\n",
        "\n",
        "#Train Random forest to compare how it does against SVM\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "#clf = RandomForestClassifier(n_estimators = 100, random_state=30)\n",
        "#clf.fit(im_features, np.array(image_classes))\n",
        "\n",
        "# Save the SVM\n",
        "#Joblib dumps Python object into one file\n",
        "import joblib\n",
        "joblib.dump((clf, training_names, stdSlr, k, voc), \"bovw.pkl\", compress=3)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lv_HQC5ZCzr"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pylab as pl\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score #sreeni\n",
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-ELebgHZCzs"
      },
      "source": [
        "# Load the classifier, class names, scaler, number of clusters and vocabulary \n",
        "#from stored pickle file (generated during training)\n",
        "clf, classes_names, stdSlr, k, voc = joblib.load(\"bovw.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk-sOk-SZCzt"
      },
      "source": [
        "# Get the path of the testing image(s) and store them in a list\n",
        "#test_path = 'dataset/test' # Names are Aeroplane, Bicycle, Car\n",
        "test_path = r'/content/drive/MyDrive/Colab Notebooks/DATA SOURCE/PPE/test'\n",
        "\n",
        "\n",
        "testing_names = os.listdir(test_path)\n",
        "\n",
        "# Get path to all images and save them in a list\n",
        "# image_paths and the corresponding label in image_paths\n",
        "image_paths = []\n",
        "image_classes = []\n",
        "class_id = 0\n",
        "\n",
        "#To make it easy to list all file names in a directory let us define a function\n",
        "#\n",
        "def imglist(path):\n",
        "    return [os.path.join(path, f) for f in os.listdir(path)]\n",
        "\n",
        "#Fill the placeholder empty lists with image path, classes, and add class ID number\n",
        "\n",
        "for testing_name in testing_names:\n",
        "    dir = os.path.join(test_path, testing_name)\n",
        "    class_path = imglist(dir)\n",
        "    image_paths+=class_path\n",
        "    image_classes+=[class_id]*len(class_path)\n",
        "    class_id+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZmuPPiZCzu"
      },
      "source": [
        "# Create feature extraction and keypoint detector objects\n",
        "    #SIFT is not available anymore in openCV    \n",
        "# Create List where all the descriptors will be stored\n",
        "des_list = []\n",
        "\n",
        "#BRISK is a good replacement to SIFT. ORB also works but didn;t work well for this example\n",
        "brisk = cv2.BRISK_create(30)\n",
        "\n",
        "for image_path in image_paths:\n",
        "    im = cv2.imread(image_path)\n",
        "    kpts, des = brisk.detectAndCompute(im, None)\n",
        "    des_list.append((image_path, des))   \n",
        "    \n",
        "# Stack all the descriptors vertically in a numpy array\n",
        "descriptors = des_list[0][1]\n",
        "for image_path, descriptor in des_list[0:]:\n",
        "    descriptors = np.vstack((descriptors, descriptor)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnQIsrg2ZCzv"
      },
      "source": [
        "# Calculate the histogram of features\n",
        "#vq Assigns codes from a code book to observations.\n",
        "from scipy.cluster.vq import vq    \n",
        "test_features = np.zeros((len(image_paths), k), \"float32\")\n",
        "for i in range(len(image_paths)):\n",
        "    words, distance = vq(des_list[i][1],voc)\n",
        "    for w in words:\n",
        "        test_features[i][w] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N8O6n_4ZCzv"
      },
      "source": [
        "# Perform Tf-Idf vectorization\n",
        "nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
        "idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
        "\n",
        "# Scale the features\n",
        "#Standardize features by removing the mean and scaling to unit variance\n",
        "#Scaler (stdSlr comes from the pickled file we imported)\n",
        "test_features = stdSlr.transform(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geOqAQ14ZCzx"
      },
      "source": [
        "#######Until here most of the above code is similar to Train except for kmeans clustering####\n",
        "\n",
        "#Report true class names so they can be compared with predicted classes\n",
        "true_class =  [classes_names[i] for i in image_classes]\n",
        "# Perform the predictions and report predicted class names. \n",
        "predictions =  [classes_names[i] for i in clf.predict(test_features)]\n",
        "\n",
        "\n",
        "#Print the true class and Predictions \n",
        "print (\"true_class =\"  + str(true_class))\n",
        "print (\"prediction =\"  + str(predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kiss9idZCzy"
      },
      "source": [
        "###############################################\n",
        "#To make it easy to understand the accuracy let us print the confusion matrix\n",
        "\n",
        "def showconfusionmatrix(cm):\n",
        "    pl.matshow(cm)\n",
        "    pl.title('Confusion matrix')\n",
        "    pl.colorbar()\n",
        "    pl.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn_xZWTNZCzy"
      },
      "source": [
        "accuracy = accuracy_score(true_class, predictions)\n",
        "print (\"accuracy = \", accuracy)\n",
        "cm = confusion_matrix(true_class, predictions)\n",
        "print (cm)\n",
        "\n",
        "showconfusionmatrix(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}